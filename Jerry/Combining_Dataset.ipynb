{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "csv.field_size_limit(100000000)"
   ]
  },
  {
   "source": [
    "## Processing https://www.kaggle.com/jruvika/fake-news-detection Dataset into Combined Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "jruvika_csv = pd.read_csv('./work/Datasets/Kaggle/Dataset_1/data.csv', engine='python')\n",
    "jruvika_csv.columns = ['url', 'title', 'content', 'label']\n",
    "jruvika_csv['label'] = np.where(jruvika_csv['label'] == 1, 'reliable', 'fake')\n",
    "\n",
    "# Extracting out domain from URL\n",
    "def extract_domain(url):\n",
    "    partialDomain = urlparse(url).netloc\n",
    "    return partialDomain.replace(\"www.\", \"\")\n",
    "jruvika_csv['domain'] = jruvika_csv['url'].apply(lambda x: extract_domain(x))\n",
    "\n",
    "jruvika_csv\n",
    "\n",
    "# Saving to CSV\n",
    "jruvika_csv.to_csv('./work/Datasets/Combined_Dataset/combined_data.csv', mode='a', index=False)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "source": [
    "## Processing LIAR dataset\n",
    "### Some references to: https://towardsdatascience.com/identifying-fake-news-the-liar-dataset-713eca8af6ac"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "liar_csv = pd.read_table('./work/Datasets/liar_dataset/train.tsv', names = ['id', 'label', 'statement', 'subject', 'speaker', 'job', 'state', 'party',\n",
    "                                            'barely-true', 'false', 'half-true', 'mostly-true', 'pants-fire', 'venue'])\n",
    "\n",
    "liar_csv = liar_csv.drop(columns=['id', 'subject', 'job', 'state', 'party', 'barely-true', 'false', 'half-true', 'mostly-true', 'pants-fire', 'venue'])\n",
    "\n",
    "liar_label_dict = {\n",
    "    \"pants-fire\" : 'fake', \n",
    "    \"false\" : 'fake', \n",
    "    \"barely-true\" : 'fake', \n",
    "    \"half-true\" : 'fake', \n",
    "    \"mostly-true\" : 'reliable', \n",
    "    \"true\" : 'reliable'\n",
    "}\n",
    "liar_csv['label'] = liar_csv['label'].apply(lambda x: liar_label_dict[x])\n",
    "\n",
    "\n",
    "liar_csv.to_csv('./work/Datasets/Combined_Dataset/truncated_liar_dataset.csv', mode='a', index=False)\n",
    "print(\"Done\")"
   ]
  },
  {
   "source": [
    "## Processing Fakenewscorpus Datas into Combined Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FINSIHED WITH CORPUS DATA\n"
     ]
    }
   ],
   "source": [
    "fakenewscorpus_csv_iter = pd.read_csv('./work/Datasets/FakeNewsCorpus/news_cleaned_2018_02_13.csv', engine='python', iterator=True, chunksize=100000, encoding='utf-8', header=0, error_bad_lines=False)\n",
    "\n",
    "label_dict = {\n",
    "    'political': 'reliable',\n",
    "    'bias': 'fake',\n",
    "    'unreliable': 'fake',\n",
    "    'reliable': 'reliable',\n",
    "    'fake': 'fake'\n",
    "}\n",
    "\n",
    "for single_df in fakenewscorpus_csv_iter:\n",
    "    single_df = single_df[(single_df.type == 'fake') | (single_df.type == 'reliable') | (single_df.type == 'political') | (single_df.type == 'bias') | (single_df.type == 'unreliable')]\n",
    "\n",
    "    single_df.dropna()\n",
    "    single_df = single_df.rename({'type': 'label', 'authors': 'author'}, axis='columns')\n",
    "    single_df = single_df.drop(columns=['id', 'scraped_at', 'inserted_at', 'updated_at', 'keywords', 'meta_keywords', 'meta_description', 'tags', 'summary'])\n",
    "\n",
    "\n",
    "    single_df['label'] = single_df['label'].apply(lambda x: label_dict[x])\n",
    "    single_df.to_csv('./work/Datasets/Combined_Dataset/truncated_corpus.csv', mode='a')\n",
    "\n",
    "print(\"FINSIHED WITH CORPUS DATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}